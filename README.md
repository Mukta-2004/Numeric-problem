# Table of Contents

- [Solution of Linear Equations](#solution-of-linear-equations)
  - [Gauss Elimination Method](#gauss-elimination-method)
    - [Theory](#gauss-elimination-theory)
    - [Code](#gauss-elimination-code)
    - [Input](#gauss-elimination-input)
    - [Output](#gauss-elimination-output)
  - [Gauss Jordan Elimination Method](#gauss-jordan-elimination-method)
    - [Theory](#gauss-jordan-theory)
    - [Code](#gauss-jordan-code)
    - [Input](#gauss-jordan-input)
    - [Output](#gauss-jordan-output)
  - [LU Decomposition Method](#lu-decomposition-method)
    - [Theory](#lu-decomposition-theory)
    - [Code](#lu-decomposition-code)
    - [Input](#lu-decomposition-input)
    - [Output](#lu-decomposition-output)
  - [Matrix Inversion](#matrix-inversion)
    - [Theory](#matrix-inversion-theory)
    - [Code](#matrix-inversion-code)
    - [Input](#matrix-inversion-input)
    - [Output](#matrix-inversion-output)

- [Solution of Non-Linear Equations](#solution-of-non-linear-equations)
  - [Bisection Method](#bisection-method)
    - [Theory](#bisection-theory)
    - [Code](#bisection-code)
    - [Input](#bisection-input)
    - [Output](#bisection-output)
  - [False Position Method](#false-position-method)
    - [Theory](#false-position-theory)
    - [Code](#false-position-code)
    - [Input](#false-position-input)
    - [Output](#false-position-output)
  - [Secant Method](#secant-method)
    - [Theory](#secant-theory)
    - [Code](#secant-code)
    - [Input](#secant-input)
    - [Output](#secant-output)
  - [Newton Raphson Method](#newton-raphson-method)
    - [Theory](#newton-raphson-theory)
    - [Code](#newton-raphson-code)
    - [Input](#newton-raphson-input)
    - [Output](#newton-raphson-output)

- [Solution of Differential Equations and Differentiation](#solution-of-differential-equations-and-differentiation)
  - [Runge-Kutta Method](#runge-kutta-method)
    - [Theory](#runge-kutta-theory)
    - [Code](#runge-kutta-code)
    - [Input](#runge-kutta-input)
    - [Output](#runge-kutta-output)
  - [Differentiation Method](#differentiation-method)
    - [Theory](#differentiation-theory)
    - [Code](#differentiation-code)
    - [Input](#differentiation-input)
    - [Output](#differentiation-output)

- [Solution of Curve Fitting](#solution-of-curve-fitting)
  - [Linear Equation](#linear-equation-method)
    - [Theory](#linear-equation-theory)
    - [Code](#linear-equation-code)
    - [Input](#linear-equation-input)
    - [Output](#linear-equation-output)
  - [Polynomial Equation](#polynomial-equation-method)
    - [Theory](#polynomial-equation-theory)
    - [Code](#polynomial-equation-code)
    - [Input](#polynomial-equation-input)
    - [Output](#polynomial-equation-output)
  - [Transcendental Equation](#transcendental-equation-method)
    - [Theory](#transcendental-equation-theory)
    - [Code](#transcendental-equation-code)
    - [Input](#transcendental-equation-input)
    - [Output](#transcendental-equation-output)

- [Solution of Interpolation and Approximation](#solution-of-interpolation-and-approximation)
  - [Newton's Forward Interpolation](#newton-forward-interpolation)
    - [Theory](#newton-forward-interpolation-theory)
    - [Code](#newton-forward-interpolation-code)
    - [Input](#newton-forward-interpolation-input)
    - [Output](#newton-forward-interpolation-output)
  - [Newton's Backward Interpolation](#newton-backward-interpolation)
    - [Theory](#newton-backward-interpolation-theory)
    - [Code](#newton-backward-interpolation-code)
    - [Input](#newton-backward-interpolation-input)
    - [Output](#newton-backward-interpolation-output)
  - [Newton's Divided Difference Interpolation](#newton-divided-difference-interpolation)
    - [Theory](#newton-divided-difference-interpolation-theory)
    - [Code](#newton-divided-difference-interpolation-code)
    - [Input](#newton-divided-difference-interpolation-input)
    - [Output](#newton-divided-difference-interpolation-output)

- [Solution of Numerical Integration](#solution-of-numerical-integration)
  - [Simpson's One-Third Rule](#simpson-one-third-rule)
    - [Theory](#simpson-one-third-rule-theory)
    - [Code](#simpson-one-third-rule-code)
    - [Input](#simpson-one-third-rule-input)
    - [Output](#simpson-one-third-rule-output)
  - [Simpson's Three-Eighth Rule](#simpson-three-eighth-rule)
    - [Theory](#simpson-three-eighth-rule-theory)
    - [Code](#simpson-three-eighth-rule-code)
    - [Input](#simpson-three-eighth-rule-input)
    - [Output](#simpson-three-eighth-rule-output)

---


### Solution of Linear Equations

### Gauss Elimination Method

#### Gauss Elimination Theory
[Add your theory content here]

#### Gauss Elimination Code
```python
# Add your code here
```

#### Gauss Elimination Input
```
[Add your input format here]
```

#### Gauss Elimination Output
```
[Add your output format here]
```

---

### Gauss Jordan Elimination Method

#### Gauss Jordan Theory
[Add your theory content here]

#### Gauss Jordan Code
```python
# Add your code here
```

#### Gauss Jordan Input
```
[Add your input format here]
```

#### Gauss Jordan Output
```
[Add your output format here]
```

---

### LU Decomposition Method

#### LU Decomposition Theory
[Add your theory content here]

#### LU Decomposition Code
```python
# Add your code here
```

#### LU Decomposition Input
```
[Add your input format here]
```

#### LU Decomposition Output
```
[Add your output format here]
```

---

### Matrix Inversion

#### Matrix Inversion Theory
[Add your theory content here]

#### Matrix Inversion Code
```python
# Add your code here
```

#### Matrix Inversion Input
```
[Add your input format here]
```

#### Matrix Inversion Output
```
[Add your output format here]
```

---

### Solution of Non-Linear Equations

### Bisection Method

#### Bisection Theory
[Add your theory content here]

#### Bisection Code
```python
# Add your code here
```

#### Bisection Input
```
[Add your input format here]
```

#### Bisection Output
```
[Add your output format here]
```

---

### False Position Method

#### False Position Theory
[Add your theory content here]

#### False Position Code
```python
# Add your code here
```

#### False Position Input
```
[Add your input format here]
```

#### False Position Output
```
[Add your output format here]
```
### Secant Method
#### Secant Theory
[Add your theory content here]
#### Secant Code
```python
# Add your code here
```
#### Secant Input
```
[Add your input format here]
```
#### Secant Output
```
[Add your output format here]
```

### Newton Raphson Method
#### Newton Raphson Theory
[Add your theory content here]
#### Newton Raphson Code
```python
# Add your code here
```
#### Newton Raphson Input
```
[Add your input format here]
```
#### Newton Raphson Output
```
[Add your output format here]
```

### Solution of Differential Equations and Differentiation
### Runge-Kutta Method
#### Runge-Kutta Theory
[Add your theory content here]
#### Runge-Kutta Code
```python
# Add your code here
```
#### Runge-Kutta Input
```
[Add your input format here]
```
#### Runge-Kutta Output
```
[Add your output format here]
```

### Differentiation Method
#### Differentiation Theory
[Add your theory content here]
#### Differentiation Code
```python
# Add your code here
```
#### Differentiation Input
```
[Add your input format here]
```
#### Differentiation Output
```
[Add your output format here]
```

---

## Solution of Curve Fitting

### Linear Equation

#### Linear Equation Theory
Introduction:

Linear Regression is a numerical method used to determine the best fitting straight line between two variables x and y. The relationship is represented by a linear equation:

y = a + b*x

where a is the intercept and b is the slope of the line.

Working Principle:

The method uses the Least Squares approach to minimize the total squared error between observed data points and estimated values. Given n data points, required sums are calculated from x and y values. The slope and intercept are obtained using:

b = (n*sum(x*y) - sum(x)sum(y)) / (n*sum(x*x) - (sum(x))^2) ; 

a = (sum(y) - b*sum(x)) / n

Using these values, the best fit linear equation is formed.

Special Cases:
1. If all x values are equal, the denominator becomes zero and regression is not possible.
2. At least two data points are required.
3. If data points lie exactly on a straight line, the regression gives a perfect fit.

Advantages:
1. Simple and easy to implement.
2. Requires less computation.
3. Works well for large datasets.
4. Useful for prediction and trend analysis.

Best and Worst Use Cases:

Works Best When:
1. Data follows an approximately linear pattern
2. Outliers are minimal
   
Works Worst When:
1. Data is highly nonlinear
2. Extreme outliers are present
3. Very small datasets

Conclusion:
Linear Regression is a fundamental numerical technique for modeling linear relationships. While easy to apply and efficient, its accuracy depends on the nature and quality of the data.

#### Linear Equation Code
```python
#include<bits/stdc++.h>
#include <fstream>
using namespace std;

int main()
{
    string ifile = "InputLinear.txt";
    string ofile = "OutputLinear.txt";

    ifstream in(ifile);
    if (!in)
    {
        cout << "Error opening input file"<<endl;
        return 1;
    }
    int n;
    in >> n;
    vector<double> x(n), y(n);
    for (int i = 0; i < n; i++)
    {
        in >> x[i];
    }
    for (int i = 0; i < n; i++)
    {
        in >> y[i];
    }
    in.close();
    double sumx = 0;
    double sumy = 0;
    double sumxy = 0;
    double sumx2 = 0;

    for (int i = 0; i < n; i++)
    {
        sumx+= x[i];
        sumy+= y[i];
        sumxy+= x[i]*y[i];
        sumx2+= x[i]*x[i];
    }
    double b = (n*sumxy-sumx*sumy)/(n*sumx2-sumx*sumx);
    double a = (sumy-b*sumx)/n;

    ofstream out(ofile);
    if (!out)
    {
        cout << "Error opening output file"<<endl;
        return 1;
    }
    out << fixed << setprecision(4);
    out << "Number of data points: " << n <<endl;
    out << "x values:\n";
    for (int i = 0; i < n; i++)
    {
        out << x[i] << " ";
    }
    out <<endl;
    out << "y values:\n";
    for (int i = 0; i < n; i++)
    {
        out << y[i] << " ";
    }
    out << endl;
    out << "y = " << a << " + " << b << "x"<<endl;
    out.close();
    return 0;
}

```

#### Linear Equation Input
```
5
1 2 3 4 5
2 4 5 4 5

```

#### Linear Equation Output
```
Number of data points: 5
x values:
1.0000 2.0000 3.0000 4.0000 5.0000 
y values:
2.0000 4.0000 5.0000 4.0000 5.0000 
y = 2.2000 + 0.6000x

```

---

### Polynomial Equation

#### Polynomial Equation Theory
[Add your theory content here]

#### Polynomial Equation Code
```python
# Add your code here
```

#### Polynomial Equation Input
```
[Add your input format here]
```

#### Polynomial Equation Output
```
[Add your output format here]
```

---

### Transcendental Equation
#### Transcendental Equation Theory
Introduction:

Transcendental Regression is used when the relationship between variables is non-linear but can be converted into a linear form using mathematical transformations. This method applies linear regression after transforming the given data.
In this program, two models are supported:

Power model: y = a*x^b

Exponential model: p = p0 * e^(k*t)


Working Principle:

The given nonlinear equations are converted into linear equations using logarithms.

For the power model: log(y) = log(a) + b * log(x)

For the exponential model: log(p) = log(p0) + k * t

After transformation, the variables become linear and standard least squares linear regression is applied. The slope and intercept are calculated, and the original constants are recovered using exponential functions.

Special Cases:
1. All y values must be positive because logarithm of zero or negative numbers is undefined.
2. For the power model, x values must also be positive.
3. At least two data points are required.
4. If transformed data is perfectly linear, the regression gives an exact fit.

Advantages:
1. Allows regression for nonlinear models.
2. Uses simple linear regression after transformation.

Best and Worst Use Cases:

Works Best When:

1. Data follows power or exponential behavior.
2. All data values are positive.
3. Noise in data is minimal.
   
Works Worst When:

1. Data contains zero or negative values
2. Relationship cannot be linearized
3. Dataset is very small or highly noisy

Conclusion:

Transcendental Regression extends linear regression to nonlinear equations by using transformations. It is effective for power and exponential models but requires careful handling of data values and assumptions.
#### Transcendental Equation Code
```python
#include<bits/stdc++.h>
#include<fstream>
using namespace std;

int main()
{
    string inputFile = "InputTranscendental.txt";
    string outputFile = "OutputTranscendental.txt";

    ifstream in("InputTranscendental.txt");
    if (!in )
    {
        cout << "Input file error"<<endl;
        return 1;
    }

    int choice, n;
    in >> choice;// 1-> y = ax^b ; 2-> p = p0*e^(kt)
    in >> n;
    vector<double> x(n), y(n);
    for (int i = 0; i < n; i++)
    {
        in >> x[i];
    }
    for (int i = 0; i < n; i++)
    {
        in >> y[i];
    }
    in.close();

    double sx = 0;
    double sy = 0;
    double sxx = 0;
    double sxy = 0;

    for (int i = 0; i < n; i++)
    {
        double X, Y;
        Y = log(y[i]);
        if (choice == 1)
        {
            X = log(x[i]);
        }
        else if(choice == 2)
        {
            X = x[i];
        }
        else
        {
            cout << "Invalid choice"<<endl;
        }
        sx  += X;
        sy  += Y;
        sxx += X*X;
        sxy += X*Y;
    }

    double b = (n*sxy-sx*sy)/(n*sxx-sx*sx);
    double A = (sy-b*sx)/n;
    ofstream out(outputFile);
    if (!out)
    {
        cout << "Output file error"<<endl;
        return 1;
    }
    out << fixed << setprecision(6);
    out << "Number of data points: " << n <<endl;
    out << "x values:"<<endl;
    for (int i = 0; i < n; i++)
    {
        out << x[i] << " ";
    }
    out <<endl;
    out << "y values:"<<endl;
    for (int i = 0; i < n; i++)
    {
        out << y[i] << " ";
    }
    out<<endl;
    if (choice == 1)
    {
        double a = exp(A);
        out << "y = " << a << " * x^(" << b << ")"<<endl;
    }
    else if (choice == 2)
    {
        double p0 = exp(A);
        double k  = b;
        out << "p = " << p0 << " * e^(" << k << " t)"<<endl;
    }
    else
    {
        out << "Invalid choice"<<endl;
    }
    out.close();
    return 0;
}


```
#### Transcendental Equation Input
```
1
4
1 2 3 4
2.7 7.4 20.1 54.6

```
#### Transcendental Equation Output
```
Number of data points: 4
x values:
1.000000 2.000000 3.000000 4.000000 
y values:
2.700000 7.400000 20.100000 54.600000 
y = 2.276154 * x^(2.109951)

```
### Solution of Interpolation and Approximation
### Newton's Forward Interpolation
#### Newton's Forward Interpolation Theory
[Add your theory content here]
#### Newton's Forward Interpolation Code
```python
#Add your code here
```
#### Newton's Forward Interpolation Input
```
[Add your input format here]
```
#### Newton's Forward Interpolation Output
```
[Add your output format here]
```

### Newton's Backward Interpolation
#### Newton's Backward Interpolation Theory
[Add your theory content here]
#### Newton's Backward Interpolation Code
```python
#Add your code here
```
#### Newton's Backward Interpolation Input
```
[Add your input format here]
```
#### Newton's Backward Interpolation Output
```
[Add your output format here]
```

### Newton's Divided Difference Interpolation
#### Newton's Divided Difference Interpolation Theory
[Add your theory content here]
#### Newton's Divided Difference Interpolation Code
```python
#Add your code here
```
#### Newton's Divided Difference Interpolation Input
```
[Add your input format here]
```
#### Newton's Divided Difference Interpolation Output
```
[Add your output format here]
```

### Solution of Numerical Integration
### Simpson's One-Third Rule
#### Simpson's One-Third Rule Theory
[Add your theory content here]
#### Simpson's One-Third Rule Code
```python
#Add your code here
```
#### Simpson's One-Third Rule Input
```
[Add your input format here]
```
#### Simpson's One-Third Rule Output
```
[Add your output format here]
```
### Simpson's Three-Eighth Rule
#### Simpson's Three-Eighth Rule Theory
[Add your theory content here]
#### Simpson's Three-Eighth Rule Code
```python
#Add your code here
```
#### Simpson's Three-Eighth Rule Input
```
[Add your input format here]
```
#### Simpson's Three-Eighth Rule Output
```
[Add your output format here]
```



---





